% !TEX root = ../entropy.tex

\section{Methods}%
\label{sec:methods}


\subsection{Dataset description}
\label{par:dataset_description}

We use data from Money Dashboard (MDB), a financial management app that allows
its users to link accounts from different banks to obtain an integrated view of
their
finances.\footnote{\href{https://www.moneydashboard.com}{https://www.moneydashboard.com}.}
The dataset contains more than 500 million transactions made between 2012 and
June 2020 by about 250,000 users, and provides information such as date,
amount, and description about the transaction as well as account and user-level
information.

The main advantages of the data for the study of consumer financial behaviour
are its high frequency, that it is automatically collected and updated and thus
less prone to errors and unaffected by biases that bedevil survey measures, and
that it offers a view of consumers' entire financial life across all their
accounts, rather than just a view of their accounts held at a single bank,
provided they added all their accounts to MDB. The main limitation is the
non-representativeness of the sample relative to the population as a whole.
Financial management apps are known to be used disproportionally by men,
younger people, and people of higher socioeconomic status
\citep{carlin2019generational}. Also, as pointed out in
\citet{gelman2014harnessing}, a willingness to share financial information with
a third party might not only select on demographic characteristics, but also
for an increased need for financial management or a higher degree of financial
sophistication. Because our analysis does not rely on representativeness, we do
not address this.\footnote{For an example of how re-weighing can be used to
mitigate the non-representative issue, see \citet{bourquin2020effects}.}


\paragraph{Data issues}%
\label{par:data_issues}

\citet{bourquin2020effects} argue that because some of the accounts in the data
will be joint accounts, units of observations should be tought of as
"households" rather than "users". We do not agree that this is the most prudent
approach. The validity of thinking of units as households depends on the
proportion of users in the data who add joint accounts and on the proportion of
transactions -- out of a user's total number of transactions -- additionally
observed as a result. Given that the sample is skewed towards younger
individuals we think it is unlikely that a majority of them has added joint
accounts. Furthermore, it seems reasonable to assume that in most cases, joint
accounts are mainly used for common household expenditures similar that are
similar to those of a single user (albeit in higher amounts), and are thus
unlikely to alter the observed spending profile much. Thus, we think of units
of observations as individuals, not households. 

Some accounts might be business accounts. Using versions of the algorightms
used by \citet{bourquin2020effects} to identify such accounts showed, however,
that such accounts only make up a tiny percentage of overall accounts and would
not influence our results. We thus do not exclude them.



\subsection{Preprocessing and sample selection}%
\label{par:preprocessing_and_sample_selection}

We restrict our sample to users for whom we can observe a regular income, can
be reasonably sure that they have added all their bank account to MDB, and for
whom we observe at least six months of data. Table~\ref{tab:selection}
summarises the sample selection steps we applied to a 1 percent sample of the
raw data, associated data losses, and the size of our final sample. A detailed
description of the entire data cleaning and selection process is provided in
Appendix~\ref{sec:data}.

\begin{table}[H]
\centering
\caption{Sample selection}\label{tab:selection}
\input{\tabdir/sample_selection.tex}
\end{table}



\subsection{Dependent variables}
\label{par:outcome_variable_}

Identifying savings transactions: We classify as payments into savings accounts
all savings account credits of \pounds5 or more that are not identified as
interest payments or automated "save the change" transfers (similarly for
debits).\footnote{While standing order transactions are unlikely to be related
to entropy in the short-run, we do not exclude such transactions since, best we
can tell, the only account for a small fraction of total transactions.} 

Dummy for savings txn in current month. Motivation: \citet{mps2018building} finds that
saving habit is often more important than amount saved.

Gross and net inflows into savings accounts in current month. We winsorise the
top end of the distribution at the 1 percent level.




\subsection{Spending profile}%
\label{par:variable_of_interest_}

\begin{itemize}
    \item Two ways to characterise spend profile: intra-period and
        inter-period. For proof-of-concept study, we focus on former, and on
        calendar month as period.\footnote{For intra-period characterisation,
        consistency over time will be particularly interesting. Might be able
    to capture this using Jensen-Shannon divergence.}
\end{itemize}


Measures of spending profiles:
\begin{itemize}
    \item Tag-based entropy

    \item Smoothed tag-based entropy

    \item Grocery-shops based entropy

    \item Composite measure based on PCA from combination of base entropy
        scores (similar to \citet{eagle2010network}
\end{itemize}


\paragraph{Tag-based entropy}%
\label{par:tag_based_entropy}

Our variable of interest is spending entropy, a measure of how predictable an
individual's spending pattern is at a given point in time, which we interpret
more broadly as a measure of the degree to which an individual's life is
chaotic. Entropy is a cornerstone of information theory, where it measures the
amount of information contained in an event. In the behavioural sciences,
behavioural entropy has recently been shown to predict the frequency of grocery
visits and the per-capita spend per visit \citep{guidotti2015behavioral}, the
amount of calories consumed \citep{skatova2019those}, and the propensity for
financial distress \citep{muggleton2020evidence}.

We calculate spending entropy using the formula proposed by
\citet{shannon1948mathematical}, which defines entropy as:\footnote{Shannon
    entropy is customarily denoted as $H$ following Shannon's own naming after
    Ludwig Boltzman's 1872 H-theorem in statistical mechanics, to which it is
analagous.}

\begin{equation}
\label{equ:entropy}
    H = -\sum{p_i}log(p_i),
\end{equation}

where $p_i$ is the probability that an individual makes a purchase in spending
category $i$, and $log$ is the base 2 logarithm.

\edit{We normalise $H$ by $log(N_{SC})$, the entropy of completely random shopping
    behaviour, so that it takes value between 0 and 1.\footnote{$log(N_{SC})$ is
    the probability of a completely random shopping pattern because for in this
case, for $N_{SC}$ different spending categories, we would have $p_i = 1 /
N_{SC}$ for each category $i$ so that $H = -N_{SC}p_ilog(p_i) = -log(p_i) =
log(N_{SC})$.}}

The higher the value of
entropy, the less predictable an individuals spending pattern.

To calculate entropy scores, we group spending into 9 spending categories (SC)
based on the classification used by Lloyds Banking Group as discussed in
\citet{muggleton2020evidence}. Also following that paper, we use additive
smoothing to calcualte probabilities to avoid taking logs of zeroes in cases
where an individual makes no purchases in a given spending category. We thus calculate $p_i$ as: 

\begin{equation}
    p_i = \frac{\text{Count of purchases in $SC_i$} + 1}{\text{Count of
    all purchases} + N_{SC}},
\end{equation}

where $N_{SC}$ is the total number of categories.

Issues from imperfect labelling of MDB data:
\begin{itemize}

    \item Transaction tagging in the MDB data is imperfect: about 20 percent of
        transactions have no tag.

    \item This creates two issues for entropy calculation.

    \item First, entropy scores of high-entropy individuals are biased
        downwards - relatively more than those of low-entropy individuals.
        Reason: missing tags are not random: more common transactions such as
        groceries or take-away purchases are more likely to be tagged ...

    \item All zero count-cases: ... Solution: require minimum number of txns in
        two different labels (for all categories we use to calculate entropy).
        Two to avoid 0 entropy cases that are unlikely to be genuine but
        probably artefacts of missing labelling.
        
\end{itemize}

\paragraph{Auto-tag-based entropy}
\label{par:auto_tag_entropy}

\begin{itemize}
    \item Use apriori algorithm

    \item minsup: minimum number of baskets a pattern is required to appear
        (else it's dropped)

    \item In our context: baseket is collection of auto tags with positive txsn
        counts in a user month, while pattern is pattern of such auto tags.

    \item Patterns also called 'representative baskets'.

    \item Algorithm steps (adapted from guidotti2015behavioural)

        \begin{itemize}
            \item Identify all patterns (representative baskets)

            \item Discard representative baskets that appear in fewer than
                minsup months we observe for a user. 

            \item Assign representative basked to each of the user's months.

            \item Calculate probabilities of observing a representative basked
                based on occurrences across all of a user's month. E.g. user
                with 5 months of data with representative baskets [1, 1, 2, 3,
                4] has representative basket probabilitis 2/5 for repr basket
                1, and 1/5 for repr baskets 2-4.

            \item Calculate user-leven entropy based on probabilities.
        \end{itemize}
\end{itemize}


\paragraph{Shopping-time based entropy}
\label{par:shopping_time_based_entropy}

We calculate entropy based on the probability of $(day of week, merchant)$
tuples, where we follow \citet{guidotti2015behavioral} and bin \textit{day of
week} into \textit{weekends} and \textit{weekday}, to reduce excessive
fluctuations. Because banks tend to process weekend transactions on Monday, as
shown in
Figure~\ref{fig:spending}, we cannot distinguish transactions made on Saturdays
or Sundays from those made on Mondays, and thus classify all of them as weekend
transactions.

We drop the about 25 percent of transactions for which we cannot identify a
merchant. The alternative would be leaving these transactions in the sample and
treating ``unknown merchant'' as a single merchant. But for user-months for
which the merchant is unknown for all transactions, this would lead to an
entropy score of 0, which is undesireable.

\paragraph{Grocery shop entropy}%
\label{par:grocery_shop_entropy}

We consider purchases at Tesco, Sainsbury's, Asda, Morrisons, Aldi, Co-op,
Lidl, Waitrose, Iceland, and Ocado, which have a combined market share of 96.5
percent.

\paragraph{Additional entropy notes}%

In equation~\ref{equ:entropy} we have defined entropy as $H =
-\sum{p_i}log(p_i)$ and pointed out that it can loosely be interpreted as the
predictability of an individual's spending behaviour. In this section, we
provide a more detailed discussion of the formula.

The building blocks of entropy is the information content of a single event.
The key intuition \citet{shannon1948mathematical} aimed to capture was that
learning of the occurrence of a low-probability event is more informative than
learning of the occurrence of a high-probability event. The information of an
event $I(E)$ is thus inversely proportional to is probability $p(E)$. One way
to capture this would be to define the information of event E as $I(E) =
\frac{1}{p(E)}$. Yet this implied that an event that is certain to occur had
information 1, when it would make sense to have information 0. To remedy this
(and also satisfy additional desireable characteristics of an information
function), we can can use the log of the expression. Hence, the information of
event E, often called \textit{Shannon information}, \textit{self-information},
or just \textit{information}, is defined as:

\begin{equation}
    I(E) = log\left(\frac{1}{p(E)}\right) = -log(p(E))
\end{equation}

Entropy, often called \textit{Information entropy}, \textit{Shannon entropy},
or just \textit{entropy}, is the information of a random variable and captures
the expected amount of information of an event drawn at random from the
probability distribution of the random variable. It is calcualted as:

\begin{equation}
    H(X) = -\sum_x p(x) \times log(p(x)) = \sum_x p(x)I(x) = \mathbb{E} I(x).
\end{equation}

% For a single event, the key intution was that the less likely an event, the
% more information is conveyed when it occurs. The related idea for distributions
% is similar: the more skewed a distribution, the more uncertain the autcome, the
% higher its entropy. The minimal entropy distribution is thus the uniform
% distribution.

\edit{todo: Discuss link to spending behaviour}
% --todo-- correct the above, it's incorrect. uniform is max entropy
% distribution. Link to spending behaviour.



% -p Why does entropy (when calculated using base 2 logarithms) also represent the number of bits required to convey the average outcome of a distribution? [Wikipedia](https://en.wikipedia.org/wiki/Entropy_&28information_theory%29#Introduction) (in the second to last paragraph of the introduction) explains this well. Basically, its because if there are some events with very high probability, then these events could be transmitted with short codes of only a few bits, so that most of the time, only a few bits have to be transmitted to send the message.

% The choice of the base for the
% logarithm varies by application and determines the units of $I(E)$. Base 2
% means that information is expressed in bits. The natural logarithm, another
% popular choice, expresses information in \textit{nats}.

\paragraph{Entropy calculation}%
\label{sub:entropy_calculation}


Entropy can be calculated along a number of dimensions.

\begin{itemize}
    \item Category-based vs time-based vs category-time based
        \citep{guidotti2015behavioral, krumme2013predictability}

    \item Count-based vs value-based

    \item Intratemporal vs intertemporal \citep{krumme2013predictability}

        \begin{itemize}
            \item Based on behaviour within a given time period or changes in
                behaviour across time periods.
        \end{itemize}
        
\end{itemize}

Desireable features of entropy variable:

\begin{itemize}
    \item Based on a large enough number of categories so that spend on many of
        them can reasonably be interpreted as chaotic (the 9 LBG tags seem
        insufficient for this, especially because most of them are vital life
        expenses). Use of auto tags or merchant seems preferable.

    \item 
\end{itemize}







\subsection{Control variables}%
\label{sub:control_variables}

We classify potential determinants of savings behaviour into \textit{financial behaviours},
\textit{financial planning}, and \textit{individual or household
characteristics}, a classification frequently used in policy research on
the financial wellbeing \citep{can2019improving,cfpb2017financial, mps2018building}.

Financial behaviour

\begin{itemize}
    \item Regular savings, dummy for 10 out of last 12 months

    \item Proportion of purchases paid with credit card. This is only about 6
        percent in our final sample, whereas it is 12 percent in the full
        sample.\footnote{Across the UK, the proportion of credit pard purchases
            is about 17 percent in a typical month \citep{ukfinance2021card}.
            The proportion in our data is likely lower because the sample is
        skewed towards more affluent individuals.}

    \item Month total and category spend (category spend for robustness)
\end{itemize}

Planning
\begin{itemize}
    \item Regular login, dummy for 1 / month in 10 out of last 12 months. Have
        login data for about 50 percent of sample, so best to work with full
        sample once I use it. Implement once I can do that. -- not yet
        implemented --
\end{itemize}

Individual and household characteristics
\begin{itemize}
    \item Gender

    \item Age

    \item Urban

    \item Region

    \item Year income, winsorised at the 1 percent level. We include
        year-rather than month-income because the latter can be quite variable
        (e.g. irregular work, changing jobs, etc.), and we assume people to
        base their consumption on their annual income. (Could test this in
        appendix: is correlation between annual and spend stronger than between
        monthly and spend?)

    % \item Month income, winsorised at top 1 percent-level.

    \item Regular income, dummy for 10 out of last 12 months

    \item Month income std -- not implemented yet --

    \item Income current month, dummy for month income > 0

    \item Has children, imperfect -- not yet implemented --

    \item Index of multiple deprivations from nspl -- not implemented yet --

    \item Received benefits

    \item Receives pension

    \item Housing tenure: mortgage, rent, other (owning outright implied)

    \item Takes out (payday) loan

    \item Total balance or balance / avg. month spend -- not yet implemented --
\end{itemize}




\subsection{Summary statistics}%
\label{par:summary_statistics}

Table~\ref{tab:sumstats} provides summary statistics.

\input{\tabdir/sumstats.tex}

Figure~\ref{fig:sample_desc}
\begin{figure}[H]
    \caption{Demographic characteristics of Money Dashboard users}
    \label{fig:sample_desc}
    \begin{center}
        \includegraphics[width=.9\textwidth]{\figdir/sample_desc.png}
    \end{center}
\end{figure}

Figure~\ref{fig:entropy_kdes}
\begin{figure}[H]
    \center \newcommand\width{\textwidth} \caption{Entropy distributions}
    \label{fig:entropy_kdes}
    \includegraphics[width=\width]{\figdir/entropy_kdes.png}
    \fignote{\width}{}
\end{figure}




\subsection{Model specification}%
\label{par:model_specification}

We estimate models of the form: 

\begin{equation}
    s_{i,t} = \alpha_i + \lambda_t + \beta H_{i,t} + x^\prime_{i,t} \delta +
    \epsilon_{i,t},
\end{equation}

where $s_{i,t}$ is an indicator variable equal to one if individual $i$ made
one or more transfers to any of their savings account in month $t$ and zero
otherwise, $H_{it}$ is $i$'s spending entropy in month $t$, $x_{i,t}$ a vector
of control variables, $\alpha_i$ an individual fixed effect, $\lambda_t$ a
calendar month fixed effect, and $\epsilon_{i, t}$ the error term.

Issues to think about:
\begin{itemize}
    \item Unbalanced panel is not random - people using MDB for longer are
        different. Should we just use first x months for every user? E.g. first
        year?
\end{itemize}

