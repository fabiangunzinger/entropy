% !TEX root = ../entropy.tex

\section{Methods}%
\label{sec:methods}


\subsection{Dataset description}
\label{par:dataset_description}

We use data from Money Dashboard (MDB), a financial management app that allows
its users to link accounts from different banks to obtain an integrated view of
their
finances.\footnote{\href{https://www.moneydashboard.com}{https://www.moneydashboard.com}.}
The full dataset contains more than 500 million transactions made between 2012
and June 2020 by about 270,000 users, and provides information such as date,
amount, and description about the transaction as well as account and user-level
information.

The main advantages of the data for the study of consumer financial behaviour
are its high frequency, that it can be cheaply collected for a very large
number of users, that collection is automatic and thus less prone to errors and
unaffected by biases that bedevil survey measures, and that it offers a view of
consumers' entire financial life across all their accounts, rather than just a
view of their accounts held at a single bank, provided they added all their
accounts to MDB. The main limitation is the non-representativeness of the
sample relative to the population as a whole.  Financial management apps are
known to be used disproportionally by men, younger people, and people of higher
socioeconomic status \citep{carlin2019generational}. Also, as pointed out in
\citet{gelman2014harnessing}, a willingness to share financial information with
a third party might not only select on demographic characteristics, but also
for an increased need for financial management or a higher degree of financial
sophistication. Because our analysis does not rely on representativeness, we do
not address this.\footnote{For an example of how re-weighing can be used to
mitigate the non-representative issue, see \citet{bourquin2020effects}.}


\subsection{Preprocessing and sample selection}%
\label{par:preprocessing_and_sample_selection}

The overall aim of sample selection is to restrict our sample to users for whom
we can observe a regular income, can be reasonably sure that they have added
all their bank account to MDB, and for whom we observe at least six months of
data. Table~\ref{tab:selection} summarises the sample selection steps we
applied, associated data losses, and the size of our final sample.

\begin{table}[ht]
\centering\small
\caption{Sample selection}\label{tab:selection}
\input{\tabdir/sample_selection.tex}
\tabnote{.95\textwidth}{Number of users, user-months, transactions, and
transaction volume in millions of British Pounds left in our sample after each
sample selection step.}
\end{table}

We start by dropping the first and last month of data for each user because we
are unlikely to observe users' complete data in these months, which might
affect selection for the criteria below. Next, we keep only users whom we
observe for at least 6 months to ensure that we have a suitable amount of data
for each user. We also require users to have at least one current and one
savings accounts to be able to oberve their spending and savings behaviour. The
next few steps are all aimed at selecting users about whom we can be reasonably
sure that they have added all their financial accounts. We thus select on
criteria we would expect to see for all such users: an annual income of at
least \pounds5,000, at least 10 monthly spending transaction, at least 4
monthly grocery transactions, and a total monthly spend of at least \pounds200.
Finally, we keep only users for whom we observe all relevant demographic
characteristics, and who are between 18 and 65 years old. The reason for the
last step is that younger users and retirees will plausibly have different
fianncial objectives than people in their working age, which is the population
we are interested about.\footnote{The code that implements the selection
criteria is available on
\href{https://github.com/fabiangunzinger/entropy/blob/c49c9c34c96d073725afd3a1494458a388d00051/src/data/selectors.py}{Github}.}

It is clear from the reduction in the number of users from more than 270,000 to
about 14,500 that these selection steps are restrictive. Given our aim in this
study, however, it is crucial that we can be reasonably sure that we observe
users' entire spending and savings transactions, so that the stringent criteria
are worth it. Also, size of the remaining sample is still sufficiently large
for our puposes.


\subsection{Dependent variables}
\label{sub:dependent_variables}

Our outcome variable is biniry indicator for whether of not a user made a
payment into any of their savings account in a given month. We classify as
payments into savings accounts all savings account credits of \pounds5 or
more.\footnote{While standing order transactions are unlikely to be related to
entropy in the short-run, we do not exclude such transactions since, best we
can tell, the only account for a small fraction of total transactions.}

The reason we focus on a binary indicator is twofold: first, becaus we
hypothesis that chaotic or difficult life circumstances that are also reflected
in spending entropy might make it harder to remember to save, for which the
accurate thest is to see whether spending entropy is related to the likelihood
of making ang savings transctions. Second, research by \citet{mps2018building}
suggests that to build up sufficient emergency funds over time forming a habit
of saving regularly is more important than the specific amounts saved month to
month.

\subsection{Spending profiles}%
\label{sub:spending_profiles}

We define a user's spending profile as the distribution of the number of
spending transactionas across different spend categories. To summarise these
distributions, we calculate spending entropy, based on the formula proposed by
\citet{shannon1948mathematical}, who defines entropy as $H =
-\sum{p_i}log(p_i)$, which sums, for all possible events, the product of the
probability of an event $i$ occuring with the logarithm of that
probability.\footnote{Shannon entropy is customarily denoted as $H$ following
Shannon's own naming after Ludwig Boltzman's 1872 H-theorem in statistical
mechanics, to which it is analagous.} The base of the logarithm is often chosen
to be 2, though other choices are possible. Entropy is a cornerstone of
information theory, where it measures the amount of information contained in an
event. In the behavioural sciences, behavioural entropy has recently been shown
to predict the frequency of grocery visits and the per-capita spend per visit
\citep{guidotti2015behavioral}, the amount of calories consumed
\citep{skatova2019those}, and the propensity for financial distress
\citep{muggleton2020evidence}. In our context, we define the entropy of a
user's spending profile in a particular period as (we omit individual and time
subscripts to keep the notation simpler):

\begin{equation}
\label{equ:entropy}
H = -\sum_{c \in \setc}{p_c}log(p_c),
\end{equation}

\noindent where $\setc$ is the set of all spending categories, $p_c$ the
probability that an individual makes a purchase in spending category $c$, and
$log$ the base 2 logarithm.

Higher entropy means that transactions are more equal across different spending
categories, which makes it hard to predict the next transaction, whereas low
entropy profiles have the bulk of transactions in a few dominant categories
(such as groceries and transportation) and have relatively few transactions in
other categories.\footnote{For further discussion on how to interpret
Equation~\ref{equ:entropy}, see Appendix~\ref{sec:interpreting_entropy}.} For
simpler interpretation of our regression coefficients below, we standardise
entropy scores to have a mean of 0 and a standard deviation of 1.

One slight limitation introduced by the imperfect transaction labelling in the
MDB data is that entropy scores for high-entropy individuals will be biased
downwards. This happens because unlabelled transactions tend to be transactions
that are rare (i.e. not grocery or Amazon purchases), and it is high-entropy
individuals that are more likely to engage in rare transactions. Because our
analysis mainly relies on relative entropy levels, this is not of major
consequence and we do not pursue this further.

We calculate entropy based on three sets of spend categories. The first measure
is based on 9 spending categories used by \citet{muggleton2020evidence}. The
second measure is based on our own, more fine-grained, categorisation into 48
different categories.\footnote{The precise mapping from MDB transaction tags
    into 9 and 48 categories is available on Github
    \href{https://github.com/fabiangunzinger/entropy/blob/7fa9c565bf8959ea92a9d4fe2245da0864e19c27/src/data/txn_classifications.py\#L249}{here}
    and
    \href{https://github.com/fabiangunzinger/entropy/blob/7fa9c565bf8959ea92a9d4fe2245da0864e19c27/src/data/txn_classifications.py\#L503}{here},
respectively.} The third measure is based on merchant names, as labelled by
Money Dashboard.

We also calculate spending category probabilities in two different ways. To
calculate what we call ``unsmoothed'' entropy scores, we calculate the $p_c$s
in Equation~\ref{equ:entropy} as simple frequentist probabilities

\begin{equation}
    p_c = \frac{f_c}{F},
\end{equation}

\noindent where $f_c$ is the number of transactions in spend category $c$ (the
frequency with which $c$ occurs) and $F = \sum_{c \in \setc}f_c$ the total
number of spending transactions. To avoid taking the log of zero for categories
with zero transactions, the sum in Equation~\ref{equ:entropy} is taken over
categories with positive transaction counts only.\footnote{This is
    automatically handled by the entropy
    \href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html}{implementation}
of Python's SciPy package, which is what we use to calculate entropy scores.}
To calculate ``smoothed'' entropy scores, we apply additive smoothing to
calculate propabilities as

\begin{equation}
    \label{equ:prob_s}
    p_c^{s} = \frac{f_c + 1}{F + |\setc|},
\end{equation}

\noindent where the size of set $\setc$, $|\setc|$, is the number of unique
spending categories. Hence, additive smoothing simply adds one to the numerator
and the number of unqiue spending categories to the denominator of the
unsmoothed probabilities. Because categories with a zero transaction count will
have a numerator of 1, the sum in Equation~\ref{equ:entropy} will be taken over
all categories.

To see how we can interpret entropy as the predictability of a user's spending
behaviour, it is useful to have a more complete understanding of
Equation~\ref{equ:entropy}. The building blocks of entropy is the information
content of a single event. The key intuition \citet{shannon1948mathematical}
aimed to capture was that learning of the occurrence of a low-probability event
is more informative than learning of the occurrence of a high-probability
event. The information of an event $I(E)$ is thus inversely proportional to is
probability $p(E)$. One way to capture this would be to define the information
of event E as $I(E) = \frac{1}{p(E)}$. Yet this implied that an event that is
certain to occur had information 1, when it would make sense to have
information 0. To remedy this (and also satisfy additional desireable
characteristics of an information function), Shannon proposed using the log of
the expression. Hence, the information of event E, often called \textit{Shannon
information}, \textit{self-information}, or just \textit{information}, is
defined as:

\begin{equation}
    I(E) = log\left(\frac{1}{p(E)}\right) = -log(p(E)).
\end{equation}

The choice of the base for the logarithm varies by application and determines
the units: base 2 means that information is expressed in bits; the natural
logarithm, another popular choice, expresses information in \textit{nats}.

Entropy, often called \textit{Information entropy}, \textit{Shannon entropy},
or just \textit{entropy}, is the information of a random variable, $X$, and
captures the expected amount of information of an event drawn at random from
the probability distribution of the random variable. It is calcualted as:

\begin{equation}
    H(X) = -\sum_x p(x) \times log(p(x)) = \sum_x p(x)I(x) = \mathbb{E} I(x).
\end{equation}

For a single event, the key intution was that the less likely an event, the
more information is conveyed when it occurs. The related idea for distributions
is similar: the less skewed a distribution of a random variable, the less
certain the realised value of a single draw from the distribution, the higher
is entropy - the maximum entropy distribution is the uniform distribution.

\subsection{Summary statistics}%
\label{par:summary_statistics}

Table~\ref{tab:sumstats} provides summary statistics.

\begin{table}[ht]
\centering\scriptsize
\caption{Summary statistics}
\label{tab:sumstats}
\input{\tabdir/sumstats.tex}
\end{table}


Figure~\ref{fig:sample_desc} shows sample characteristics.
\begin{figure}[ht]
    \centering
    \caption{Demographic characteristics of Money Dashboard users}
    \label{fig:sample_desc}
    \includegraphics[width=.49\textwidth]{\figdir/year_income.pdf}
    \includegraphics[width=.49\textwidth]{\figdir/year_spend.pdf}
    \includegraphics[width=.49\textwidth]{\figdir/age.pdf}
    \includegraphics[width=.49\textwidth]{\figdir/region.pdf}
    \fignote{\textwidth}{The top left and top right panels show the
        distribution of disposable income and total spending in 2019,
        respectively, benchmarked against the 2018/19 wave of the ONS Living
        Cost and Food Survey (LCFS). The bottom left panel shows the
        distribution of age, the bottom right panel that of the regions.}%
\end{figure}


\subsection{Estimation}%
\label{sub:estimation}

We estimate models of the form: 

\begin{equation}
    \label{equ:model}
    y_{i,t} = \alpha_i + \lambda_t + \beta H_{i,t} + x^\prime_{i,t} \delta +
    \epsilon_{i,t},
\end{equation}

\noindent where $y_{i,t}$ is an indicator variable equal to one if individual $i$ made
one or more transfers to any of their savings account in year-month period $t$ and zero
otherwise, $H_{it}$ is $i$'s spending entropy in year-month period $t$, $x_{i,t}$ a vector
of control variables, $\alpha_i$ an individual fixed effect, $\lambda_t$ a
year-month fixed effect, and $\epsilon_{i, t}$ the error term.

The vector of controls includes month spend, month income, an indicator for
whether a user had positive income in a given month, and income
variability, calculated as the standard deviation of month income over the
previous 12 months.

Note that while we might in principle be worried about reverse causality, since
making payments into savings accounts might lead to a non-zero count in an
additional spend category and thus change entropy, this is not a concern here.
As discussed in Section~\ref{sub:dependent_variables} and
Section~\ref{sub:spending_profiles}, we define savings as inflows into savings
accounts and define entropy based on the classification of spend transactions
on current accounts. If a user pays money from their current into one of their
savings account, such a transaction will usually be labelled in their current
account as a transfer rather than a spending transaction, and thus not enter
the calculation of their entropy score. In Appendix~\ref{sub:endogeneity}, we
provide robustness checks using lagged entropy scores, which produces very
similar results.

